{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organized-diana",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#The code below is the implementation of logistic classfication\n",
    "# We also print the training, testing accuracy and AUC value\n",
    "# the last output is the coefficients of BERT, SVM and Length because the function is Y=a*w1+b*w2+c*w3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "quarterly-technician",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def logistic_classification(): \n",
    "    X_train=pd.read_csv('train_modified.csv')\n",
    "    Y_train=pd.read_csv('train_c.csv')\n",
    "    X_test=pd.read_csv('test_modified.csv')\n",
    "    Y_test=pd.read_csv('test_c.csv')\n",
    "    classifier = LogisticRegression()\n",
    "    clf=classifier.fit(X_train,Y_train)\n",
    "    train_accuracy = clf.score(X_train, Y_train)\n",
    "    print('\\nTraining:')\n",
    "    print(' accuracy:',format( 100*train_accuracy , '.2f') ) \n",
    "    \n",
    "    print('\\nTesting: ')\n",
    "    test_predictions = clf.predict(X_test)\n",
    "    test_accuracy = clf.score(X_test, Y_test)\n",
    "    print(' accuracy:', format( 100*test_accuracy , '.2f') )\n",
    "    \n",
    "    class_probabilities = clf.decision_function(X_test)\n",
    "    test_auc_score = sklearn.metrics.roc_auc_score(Y_test, class_probabilities)\n",
    "    print(' AUC value:', format( 100*test_auc_score , '.2f') )\n",
    "    bes=clf.coef_\n",
    "    print(\"BERT                SVM               LEN\")\n",
    "    print(bes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "played-kernel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:\n",
      " accuracy: 99.91\n",
      "\n",
      "Testing: \n",
      " accuracy: 96.36\n",
      " AUC value: 99.37\n",
      "BERT                SVM               LEN\n",
      "[[ 0.86245493  2.25609201 -0.08396549]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lu/anaconda3/envs/tensorflow/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "logistic_classification()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooked-salmon",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The code below aims to calculate each news and try to find why the model decide whether it is real or fake\n",
    "#The funciton is Y=a*BERT+b*SVM+c*Length and we have know the coefficients from above\n",
    "#We just calculate the function again to get the absolute value of each term \n",
    "#The term with largest absolute value impacts result most.\n",
    "# The output below means 2094 news are decided mainly by Bert, 46 are decided by svm and none of them are decided by length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "demanding-dairy",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2094 46 0\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "bert=0.86245493\n",
    "svm=2.25609201\n",
    "length=-0.08396549\n",
    "max_w=[]\n",
    "bn=0\n",
    "sn=0\n",
    "ln=0\n",
    "with open('test_modified.csv','r') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for i,rows in enumerate(reader):\n",
    "        if i > 0:\n",
    "            w1=abs(bert*float(rows[0]))\n",
    "            w2=abs(svm*float(rows[1]))\n",
    "            w3=abs(length*float(rows[2]))\n",
    "            x=max(w1, w2, w3)\n",
    "            if(x==w1):\n",
    "                bn+=1\n",
    "                max_w.append(\"bert\")\n",
    "            if(x==w2):\n",
    "                sn+=1\n",
    "                max_w.append(\"svm\")\n",
    "            if(x==w3):\n",
    "                ln+=1\n",
    "                max_w.append(\"length\")\n",
    "print(bn, sn, ln)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exact-merit",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The two functions below was rewritten according to the new dataset\n",
    "#In these two functions, we delete some unimportant words in news like \"reuters\" and some words with encoding problems like \"채\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "current-franchise",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(string):\n",
    "    stops=set()\n",
    "    text = string.lower().split()\n",
    "    text = \" \".join(text)\n",
    "    text = re.sub(r\"http(\\S)+\", ' ', text)\n",
    "    text = re.sub(r\"www(\\S)+\", ' ', text)\n",
    "    text = re.sub(r\"&\", ' and ', text)\n",
    "    text = re.sub(\" - \", \"\", text)\n",
    "    # text = re.sub(r\"[^0-9a-zA-Z]+\", ' ', text) todo maybe should not add this line it will dramatically decrease the auc score\n",
    "    text = text.split(\"(reuters) - \")[-1].split(\"reuters\")[-1]\n",
    "    text = text.split()\n",
    "    text = [w for w in text if \"채\" not in stops]\n",
    "    text = [w for w in text if w not in stops]\n",
    "    text = \" \".join(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def extract_data_from_file(filename, upper = 100,lower = 0, index=None):\n",
    "    data = pd.read_excel(filename)\n",
    "    string_list = list()\n",
    "    output = list()\n",
    "    i=0\n",
    "    if index == None:\n",
    "        for text, label in zip(data[\"tweet\"], data[\"label\"]):\n",
    "            if (label != 0 or label != 1) and type(text) != str:\n",
    "                pass\n",
    "            else:\n",
    "                if (upper >=len(text.split()) >= lower):\n",
    "                    string_list.append(clean_text(text))\n",
    "                    if label == 1 or label ==\"fake\":\n",
    "                        output.append(0)\n",
    "                    else:\n",
    "                        output.append(1)\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    for text in string_list:\n",
    "        text = text.split()\n",
    "        text = [w for w in text if w not in stops]\n",
    "        text = \" \".join(text)\n",
    "        string_list[i]=text\n",
    "        i=i+1\n",
    "    return string_list, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "allied-cream",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The code below extract data from file and use function train_test_split to split data into X_train, X_test, Y_train, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporate-tattoo",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X, Y=extract_data_from_file(\"train.xlsx\")\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executed-breed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the latest version trianing and predicting on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "portable-evidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import csv\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn\n",
    "\n",
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "\n",
    "# ignore all future warnings\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import *\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "from nltk import word_tokenize\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import torch.optim as optim\n",
    "\n",
    "# LSTM\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "import os\n",
    "os.getcwd()\n",
    "\n",
    "# Create a bag of words (BOW) representation from text documents, using the Vectorizer function in scikit-learn\n",
    "#\n",
    "# The inputs are\n",
    "#  - a filename containing the news in xlsx format\n",
    "#  - we label all news with label == 'real' as \"1\"\n",
    "#  - we label all news with label == 'fake' as \"0\"\n",
    "#  - this creates a simple set of labels for binary classification\n",
    "#\n",
    "#  The function extracts the text and labels for each news from the xlsx data\n",
    "#  It then tokenizes and creates a sparse bag-of-words array using scikit-learn vectorizer function\n",
    "#  The number of rows in the array is the number of news with label 'real' or 'fake'\n",
    "#  The number of columns in the array is the number of terms in the vocabulary\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "\n",
    "# https://github.com/parthpatwa/covid19-fake-news-detection/blob/main/ml_baseline.ipynb\n",
    "\n",
    "def clean_text(string):\n",
    "    stops=set()\n",
    "    text = string.lower().split()\n",
    "    text = \" \".join(text)\n",
    "    text = re.sub(r\"http(\\S)+\", ' ', text)\n",
    "    text = re.sub(r\"www(\\S)+\", ' ', text)\n",
    "    text = re.sub(r\"&\", ' and ', text)\n",
    "    # text = re.sub(r\"[^0-9a-zA-Z]+\", ' ', text) todo maybe should not add this line it will dramatically decrease the auc score\n",
    "    text = text.split(\"(reuters) - \")[-1].split(\"reuters\")[-1]\n",
    "    text = text.split()\n",
    "    text = [w for w in text if \"채\" not in stops]\n",
    "    text = [w for w in text if w not in stops]\n",
    "    text = \" \".join(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def extract_data_from_file(filename, upper = 500,lower = 300, index=None):\n",
    "    data = pd.read_excel(filename)\n",
    "    string_list = list()\n",
    "    output = list()\n",
    "    i=0\n",
    "    if index == None:\n",
    "        for text, label in zip(data[\"tweet\"], data[\"label\"]):\n",
    "            if (label != 0 or label != 1) and type(text) != str:\n",
    "                pass\n",
    "            else:\n",
    "                if (upper >=len(text.split()) >= lower):\n",
    "                    string_list.append(clean_text(text))\n",
    "                    if label == 1 or label ==\"fake\":\n",
    "                        output.append(0)\n",
    "                    else:\n",
    "                        output.append(1)\n",
    "    print(len(string_list))\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    for text in string_list:\n",
    "        text = text.split()\n",
    "        text = [w for w in text if \"채\" not in stops]\n",
    "        text = [w for w in text if w not in stops]\n",
    "        text = \" \".join(text)\n",
    "        string_list[i]=text\n",
    "        i=i+1\n",
    "    print(len(string_list))\n",
    "    return string_list, output\n",
    "\n",
    "\n",
    "def plot_matrix(intersection_matrix):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    min_val, max_val = 0, 2\n",
    "\n",
    "    ax.matshow(intersection_matrix, cmap=plt.cm.Blues)\n",
    "\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            c = intersection_matrix[i, j]\n",
    "            ax.text(j, i, str(c), va='center', ha='center')\n",
    "\n",
    "\n",
    "def create_bow_from_news(filename):\n",
    "    data = pd.read_excel(filename)\n",
    "    print('Brief information of extracted data:')\n",
    "    data.info()\n",
    "    print()\n",
    "    data['tweet'] = data['tweet'].map(lambda x: clean_text(x))\n",
    "\n",
    "    print('Extracting tokens from each news.....(can be slow for a large number of news)......')\n",
    "\n",
    "    text = [news for news in data[\"tweet\"]]\n",
    "    Y = [1 if element == \"real\" else 0 for element in data[\"label\"]]\n",
    "\n",
    "    vectorizer = CountVectorizer(stop_words='english', min_df=0.01, ngram_range=(1, 2))\n",
    "    X = vectorizer.fit_transform(text)\n",
    "    print('Data shape: ', X.shape)\n",
    "\n",
    "    return X, Y, vectorizer\n",
    "\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()  # use sigmoid as a output layer here since we are using BCEloss\n",
    "\n",
    "        # Put the declaration of the RNN network here\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "\n",
    "    def forward(self, init_input, hidden):\n",
    "        combined = torch.cat((init_input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.sigmoid(output)\n",
    "        return output, hidden\n",
    "\n",
    "\n",
    "class word2VecRNN:  # Better\n",
    "    def __init__(self, train_file: str = \"Constraint_English_Train.xlsx\",\n",
    "                 test_file: str = \"Constraint_English_Val.xlsx\", input_size=120, hidden_size=150, output_size=2):\n",
    "        # Extract basic Data\n",
    "#         X_train, Y_train = extract_data_from_file(train_file)\n",
    "#         X_test, Y_test = extract_data_from_file(test_file)\n",
    "        X, Y=extract_data_from_file(\"train.xlsx\")\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(X, Y)  \n",
    "#         X_train1, Y_train1 = extract_data_from_file(\"Constraint_English_Train.xlsx\")\n",
    "#         X_train2, Y_train2 = extract_data_from_file(\"Constraint_English_Val.xlsx\")\n",
    "#         X_train = X_train1 + X_train2\n",
    "#         Y_train = Y_train1 + Y_train2\n",
    "#         X_test, Y_test = extract_data_from_file(\"english_test_with_labels.xlsx\")\n",
    "        # tokenize word\n",
    "        X_train = [word_tokenize(sentence) for sentence in X_train]\n",
    "        X_test = [word_tokenize(sentence) for sentence in X_test]\n",
    "        # eliminate stopwords\n",
    "        common_stop_words = set(stops)\n",
    "        edited_stop_words = set()  # todo edit it in the future\n",
    "        self.stop_words = common_stop_words.union(edited_stop_words)  # edit it in the future\n",
    "        self.X_train = [[word for word in sentence_list if word not in self.stop_words] for sentence_list in X_train]\n",
    "        self.X_test = [[word for word in sentence_list if word not in self.stop_words] for sentence_list in X_test]\n",
    "\n",
    "        # edit basic Y, Y_train = [torch.tensor[Fake, Real]]\n",
    "        self.Y_train = [torch.tensor([[0.0, 1.0]]) if element == 1 else torch.tensor([[1.0, 0.0]]) for element in\n",
    "                        Y_train]\n",
    "        self.Y_test = [torch.tensor([[0.0, 1.0]]) if element == 1 else torch.tensor([[1.0, 0.0]]) for element in Y_test]\n",
    "\n",
    "        # build word embedding model\n",
    "        X_all = self.X_train + self.X_test\n",
    "        self.model = Word2Vec(X_all, min_count=1, size=input_size)  # we may change size here\n",
    "        # save model\n",
    "        self.model.save('model.bin')\n",
    "        # createRNN\n",
    "        self.RNN_model = RNN(input_size, hidden_size, output_size)\n",
    "        # optimizer\n",
    "        #self.optimizer = optim.Adam(self.RNN_model.parameters(), lr=0.00009)\n",
    "        self.optimizer = torch.optim.Adagrad(self.RNN_model.parameters(), lr=0.002)\n",
    "        #self.optimizer = optim.RMSprop(self.RNN_model.parameters(),alpha=0.95, lr=0.0001)\n",
    "\n",
    "    def train_iteration_word_rnn(self, learning_rate, category_tensor, sentenceList):\n",
    "        criterion = nn.BCELoss()  # todo need to check whether need softmax layer\n",
    "        hidden = self.RNN_model.init_hidden()  # first initial is hidden\n",
    "        self.RNN_model.zero_grad()  # initialize gradient\n",
    "\n",
    "        # The forward process\n",
    "        output = 0\n",
    "        for word in sentenceList:\n",
    "            temp = self.model.wv.__getitem__(word)\n",
    "            output, hidden = self.RNN_model(torch.reshape(torch.from_numpy(temp), [1, -1]), hidden)\n",
    "\n",
    "        # change some format\n",
    "        category_tensor = torch.reshape(category_tensor, [1, 2])\n",
    "\n",
    "        # The backward process\n",
    "        loss = criterion(output, category_tensor)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()  # Does the update\n",
    "\n",
    "        # Add parameters' gradients to their values, multiplied by learning rate\n",
    "        # for p in self.RNN_model.parameters():\n",
    "        #   p.data.add_(p.grad.data, alpha=-learning_rate)  # todo maybe could use quasi Newton in the future\n",
    "\n",
    "        return output, loss.item()\n",
    "\n",
    "    def train(self, learning_rate, n_iters=6):\n",
    "        current_loss = 0\n",
    "        print_every = 1000\n",
    "        count = 0\n",
    "        for iter in range(1, n_iters + 1):  # iteration\n",
    "            for sentence_list, category_tensor in zip(self.X_train, self.Y_train):\n",
    "                output, loss = self.train_iteration_word_rnn(learning_rate, category_tensor, sentence_list)\n",
    "                current_loss += loss\n",
    "                count += 1\n",
    "\n",
    "                if count % print_every == 0:  # print loss\n",
    "                    print('Average loss: %.4f' % (current_loss / print_every))\n",
    "                    current_loss = 0\n",
    "\n",
    "        torch.save(self.RNN_model, 'word-rnn-classification.pt')  # todo Confused\n",
    "\n",
    "    def predict(self, string):\n",
    "        sentenceList = word_tokenize(string)\n",
    "        sentenceList = [word for word in sentenceList if word not in self.stop_words]\n",
    "        hidden = self.RNN_model.init_hidden()  # first initial is hidden\n",
    "        self.RNN_model.zero_grad()  # initialize gradient\n",
    "\n",
    "        # The forward process\n",
    "        output = 0\n",
    "        for word in sentenceList:\n",
    "            output, hidden = self.RNN_model(torch.reshape(torch.from_numpy(self.model.wv.__getitem__(word)), [1, -1]),\n",
    "                                            hidden)\n",
    "        # print format\n",
    "        print(\"{0}'s real news faked rate {1}\".format(string, float(output[0, 1].detach())))\n",
    "        return output\n",
    "\n",
    "    def get_predicted_possibility_list(self):\n",
    "        outputList = []\n",
    "        Y_list = []\n",
    "        for sentenceList in self.X_test: #X_train\n",
    "            hidden = self.RNN_model.init_hidden()\n",
    "            self.RNN_model.zero_grad()\n",
    "            output = 0\n",
    "            for word in sentenceList:\n",
    "                output, hidden = self.RNN_model(\n",
    "                    torch.reshape(torch.from_numpy(self.model.wv.__getitem__(word)), [1, -1]), hidden)\n",
    "            outputList.append(float(output[0, 1].detach()))\n",
    "\n",
    "        Y_list = [int(element[0, 1]) for element in self.Y_test]#self.Y_train\n",
    "\n",
    "        return outputList, Y_list\n",
    "\n",
    "    def accuracy(self):\n",
    "        output_list, Y_list = self.get_predicted_possibility_list()\n",
    "#         data = pd.read_excel(\"english_test_with_labels.xlsx\")\n",
    "#         model_save_name = \"Wrong_data_RNN.csv\"\n",
    "#         wrong_list=[]\n",
    "        k = 0\n",
    "        matrix = np.zeros((2, 2))\n",
    "        for c, v in zip(Y_list, output_list):\n",
    "            if int(c) == 1:\n",
    "                if v >= 0.5:\n",
    "                    matrix[1, 1] += 1\n",
    "                else:\n",
    "                    matrix[1, 0] += 1\n",
    "#                     wrong_list.append(k)\n",
    "            else:\n",
    "                if v <= 0.5:\n",
    "                    matrix[0, 0] += 1\n",
    "                else:\n",
    "                    matrix[0, 1] += 1\n",
    "        plot_matrix(matrix)\n",
    "        return matrix\n",
    "\n",
    "    def eval_auc(self):\n",
    "        Y_list, outputList = self.get_predicted_possibility_list()\n",
    "        return roc_auc_score(Y_list, outputList)\n",
    "\n",
    "\n",
    "class BoWsvm:\n",
    "    def __init__(self, train_file: str = \"Constraint_English_Train.xlsx\",\n",
    "                 test_file: str = \"Constraint_English_Val.xlsx\"):\n",
    "        # Extract basic Data\n",
    "#         self.X_train, self.Y_train = extract_data_from_file(train_file)\n",
    "#         self.X_test, self.Y_test = extract_data_from_file(test_file)\n",
    "#         X_train1, Y_train1 = extract_data_from_file(\"Constraint_English_Train.xlsx\")\n",
    "#         X_train2, Y_train2 = extract_data_from_file(\"Constraint_English_Val.xlsx\")\n",
    "        X, Y=extract_data_from_file(\"train.xlsx\")\n",
    "        self.X_train, self.X_test, self.Y_train, self.Y_test = train_test_split(X, Y, test_size=0.7,random_state=1)\n",
    "        print(len(self.X_test))\n",
    "\n",
    "        self.pipeline = Pipeline([\n",
    "            ('bow', CountVectorizer()), #todo may try other paramethers here\n",
    "            #(\"svm_clf\", SVC(kernel=\"sigmoid\", gamma=5, C=0.001)),\n",
    "            ('tfidf', TfidfTransformer()), #todo may use other transformer here\n",
    "            ('c', LinearSVC()) # todo can use other svc model\n",
    "        ])\n",
    "        self.pipeline.fit(self.X_train, self.Y_train)\n",
    "\n",
    "\n",
    "\n",
    "    def test(self):\n",
    "        return roc_auc_score(self.Y_test, self.pipeline.predict(self.X_test))\n",
    "    def accuracy(self):\n",
    "        output_list = self.pipeline.predict(self.X_test)\n",
    "#         data = pd.read_excel(\"english_test_with_labels.xlsx\")\n",
    "#         model_save_name = \"Wrong_data_SVM.csv\"\n",
    "#         wrong_list=[]\n",
    "        k = 0\n",
    "        matrix = np.zeros((2, 2))\n",
    "        for c, v in zip(self.Y_test, output_list):\n",
    "            if int(c) == 1:\n",
    "                if v >= 0.5:\n",
    "                    matrix[1, 1] += 1\n",
    "                else:\n",
    "                    matrix[1, 0] += 1\n",
    "#                     wrong_list.append(k)\n",
    "            else:\n",
    "                if v <= 0.5:\n",
    "                    matrix[0, 0] += 1\n",
    "                else:\n",
    "                    matrix[0, 1] += 1\n",
    "#                     wrong_list.append(k)\n",
    "            k=k+1\n",
    "        plot_matrix(matrix)\n",
    "#         df = data.iloc[wrong_list]\n",
    "#         df.to_csv(model_save_name, encoding=\"utf-8\")\n",
    "        return matrix\n",
    "\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, ntoken=2, ninp=100, nhid=256, nlayers=1, dropout=0.2):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        emb = self.drop(self.encoder(input))\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output)\n",
    "        return decoded, hidden\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters())\n",
    "        return (weight.new_zeros(self.nlayers, bsz, self.nhid),\n",
    "                weight.new_zeros(self.nlayers, bsz, self.nhid))\n",
    "\n",
    "\n",
    "def myLSTM():\n",
    "    X, Y=extract_data_from_file(\"train.xlsx\")\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y,random_state=1)\n",
    "#     X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=1)\n",
    "    X_all = X_train + X_test\n",
    "    \n",
    "    max_len = 100\n",
    "#     for sentence in X_all:\n",
    "#         new_len = len(sentence.split())\n",
    "#         if new_len > max_len:\n",
    "#             max_len = new_len\n",
    "#     max_len += 5\n",
    "    vocabulary_size = 1000 #todo test this size\n",
    "    tokenizer = Tokenizer(num_words=vocabulary_size)\n",
    "    tokenizer.fit_on_texts(X_all)\n",
    "    X_train = tokenizer.texts_to_sequences(X_train)\n",
    "    X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "    X_train, Y_train = np.array(X_train), np.array(Y_train)\n",
    "    X_test, Y_test = np.array(X_test), np.array(Y_test)\n",
    "\n",
    "    max_review_length = max_len\n",
    "    X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "    X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "\n",
    "    embedding_vector_length =100 #todo change this length\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocabulary_size, embedding_vector_length, input_length=max_review_length))\n",
    "    model.add(LSTM(40)) #todo change the parameter here\n",
    "    model.add(Dense(1, activation='sigmoid')) #todo need to test, activiation\n",
    "    model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])#todo change optimizer\n",
    "    print(model.summary())\n",
    "    history=model.fit(X_train, Y_train, epochs=2, batch_size=64,validation_data=(X_test, Y_test)) #todo change epochs, batchsize\n",
    "    # Final evaluation of the model\n",
    "    scores = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    \n",
    "    output_l= model.predict(X_test)\n",
    "    #data = pd.read_excel(\"english_test_with_labels.xlsx\")\n",
    "#     model_save_name = \"Wrong_data_LSTM.csv\"\n",
    "#     wrong_list=[]\n",
    "    k = 0\n",
    "    matrix = np.zeros((2, 2))\n",
    "    for c, v in zip(Y_test, output_l):\n",
    "        if int(c) == 1:\n",
    "            if v >= 0.5:\n",
    "                matrix[1, 1] += 1\n",
    "            else:\n",
    "                matrix[1, 0] += 1\n",
    "                #wrong_list.append(k)\n",
    "        else:\n",
    "            if v <= 0.5:\n",
    "                matrix[0, 0] += 1\n",
    "            else:\n",
    "                matrix[0, 1] += 1\n",
    "                #wrong_list.append(k)\n",
    "        #k=k+1\n",
    "    plot_matrix(matrix)\n",
    "    #df = data.iloc[wrong_list]\n",
    "    #df.to_csv(model_save_name, encoding=\"utf-8\")\n",
    "        \n",
    "    \n",
    "    \n",
    "#     print(\"Accuracy: %.2f%%\" % (scores[1] * 100))\n",
    "#     plt.plot(history.history['loss'])\n",
    "#     plt.plot(history.history['val_loss'])\n",
    "#     plt.title('model train vs validation loss')\n",
    "#     plt.ylabel('loss')\n",
    "#     plt.xlabel('epoch')\n",
    "#     plt.legend(['train','validation'], loc='upper right')\n",
    "#     plt.show()\n",
    "    \n",
    "    \n",
    "#     data = pd.read_excel(\"english_test_with_labels.xlsx\")\n",
    "#     model_save_name = \"Wrong_data_BERT.csv\"\n",
    "#     df = data.iloc[wrong_list]\n",
    "#     df.to_csv(F\"/content/gdrive/My Drive/175 BERT/{model_save_name}\", encoding=\"utf-8\")\n",
    "\n",
    "class moreFeature:\n",
    "    @staticmethod\n",
    "    def count_sentence_word_length(sentence: str):\n",
    "        return len(sentence.split())\n",
    "\n",
    "    @staticmethod\n",
    "    def count_average_char_length(sentence: str):\n",
    "        word_list = sentence.split()\n",
    "        total = 0\n",
    "        for word in word_list:\n",
    "            total += len(word)\n",
    "        return total / len(word_list)\n",
    "\n",
    "    def __init__(self, train_file: str = \"Constraint_English_Train.xlsx\",\n",
    "                 test_file: str = \"Constraint_English_Val.xlsx\"):\n",
    "        # Extract basic Data\n",
    "#         self.X_train, self.Y_train = extract_data_from_file(train_file)\n",
    "#         self.X_test, self.Y_test = extract_data_from_file(test_file)\n",
    "        X, Y=extract_data_from_file(\"train.xlsx\")\n",
    "        self.X_train, self.X_test, self.Y_train, self.Y_test = train_test_split(X, Y, test_size=0.3,random_state=1)\n",
    "        self.X_all = self.X_train + self.X_test\n",
    "        self.vectorizer = sklearn.feature_extraction.text.CountVectorizer(self.X_all, stop_words=\"english\",\n",
    "                                                                          min_df=0.01,\n",
    "                                                                          ngram_range=(1, 2))\n",
    "        self.X_all = self.vectorizer.fit_transform(self.X_all)\n",
    "        self.X_train = self.X_all[:len(self.Y_train)]\n",
    "        self.X_test = self.X_all[len(self.Y_train):]\n",
    "\n",
    "        self.classifier = linear_model.LogisticRegression(penalty=\"l2\").fit(self.X_train, self.Y_train)\n",
    "        \n",
    "\n",
    "    def accuracy_auc_report(self):\n",
    "        # Compute and print accuracy and AUC on the test data\n",
    "        print('\\nTesting: ')\n",
    "        test_predictions = self.classifier.predict(self.X_test)\n",
    "        test_accuracy = sum(test_predictions == self.Y_test) / len(self.Y_test)\n",
    "        print(' accuracy:', format(100 * test_accuracy, '.2f'))\n",
    "\n",
    "        class_probabilities = self.classifier.predict_proba(self.X_test)\n",
    "        test_auc_score = metrics.roc_auc_score(self.Y_test, class_probabilities[:, 1])\n",
    "        print(' AUC value:', format(100 * test_auc_score, '.2f'))\n",
    "\n",
    "    def most_significant_terms(self, K=10):\n",
    "        index = np.argsort(self.classifier.coef_).flatten()\n",
    "        sorted_weight = sorted(\n",
    "            self.classifier.coef_.flatten())\n",
    "\n",
    "        \n",
    "        topK_neg_weights = sorted_weight[:K]\n",
    "        topK_pos_weights = sorted_weight[-K:]\n",
    "        topK_pos_weights.reverse()\n",
    "\n",
    "        feature_name = self.vectorizer.get_feature_names()\n",
    "        sorted_vectorizer = [feature_name[i] for i in index]  # sort the vectorizer\n",
    "\n",
    "        topK_neg_features = sorted_vectorizer[:K]\n",
    "\n",
    "        topK_pos_features = sorted_vectorizer[-K:]\n",
    "        topK_pos_features.reverse()\n",
    "        print(\"Words that makes our classifier think that it is real news:\")\n",
    "        for w, f in zip(topK_pos_weights, topK_pos_features):\n",
    "            print(f, round(w, 3))  # round to 3 digits\n",
    "            ss.append(f)\n",
    "        print(\"Words that makes our classifier think that it is fake news:\")\n",
    "        for w, f in zip(topK_neg_weights, topK_neg_features):\n",
    "            print(f, round(w, 3))\n",
    "\n",
    "\n",
    "def logistic_classification(): \n",
    "    X_train=pd.read_csv('train_modified.csv')\n",
    "    Y_train=pd.read_csv('train_c.csv')\n",
    "    X_test=pd.read_csv('test_modified.csv')\n",
    "    Y_test=pd.read_csv('test_c.csv')\n",
    "    classifier = LogisticRegression()\n",
    "    clf=classifier.fit(X_train,Y_train)\n",
    "    train_accuracy = clf.score(X_train, Y_train)\n",
    "    print('\\nTraining:')\n",
    "    print(' accuracy:',format( 100*train_accuracy , '.2f') ) \n",
    "    \n",
    "    print('\\nTesting: ')\n",
    "    test_predictions = clf.predict(X_test)\n",
    "    test_accuracy = clf.score(X_test, Y_test)\n",
    "    print(' accuracy:', format( 100*test_accuracy , '.2f') )\n",
    "    \n",
    "    class_probabilities = clf.decision_function(X_test)\n",
    "    test_auc_score = sklearn.metrics.roc_auc_score(Y_test, class_probabilities)\n",
    "    print(' AUC value:', format( 100*test_auc_score , '.2f') )\n",
    "    bes=clf.coef_\n",
    "    print(\"BERT                SVM               LEN\")\n",
    "    print(bes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "stone-national",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11451\n",
      "11451\n",
      "8016\n",
      "[[4668.   34.]\n",
      " [  71. 3243.]]\n",
      "0.9869011976047904\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "        a=BoWsvm()\n",
    "        c=a.accuracy()\n",
    "        print(c)\n",
    "        print((c[0,0]+c[1,1])/(c[0,0]+c[1,1]+c[0,1]+c[1,0]))\n",
    "#     logistic_classification()\n",
    "#     bert=0.86245493\n",
    "#     svm=2.25609201\n",
    "#     length=-0.08396549\n",
    "#     max_w=[]\n",
    "#     bn=0\n",
    "#     sn=0\n",
    "#     ln=0\n",
    "#     with open('test_modified.csv','r') as csvfile:\n",
    "#         reader = csv.reader(csvfile)\n",
    "#         for i,rows in enumerate(reader):\n",
    "#             if i > 0:\n",
    "#                 w1=abs(bert*float(rows[0]))\n",
    "#                 w2=abs(svm*float(rows[1]))\n",
    "#                 w3=abs(length*float(rows[2]))\n",
    "#                 x=max(w1, w2, w3)\n",
    "#                 if(x==w1):\n",
    "#                     bn+=1\n",
    "#                     max_w.append(\"bert\")\n",
    "#                 if(x==w2):\n",
    "#                     sn+=1\n",
    "#                     max_w.append(\"svm\")\n",
    "#                 if(x==w3):\n",
    "#                     ln+=1\n",
    "#                     max_w.append(\"length\")\n",
    "#     print(\"number of news mainly predicted based on BERT, SVM, Length\")\n",
    "#     print(bn, sn, ln)\n",
    "\n",
    "\n",
    "#     a=moreFeature()\n",
    "#     a.most_significant_terms()\n",
    "\n",
    "\n",
    "#     myLSTM()\n",
    "\n",
    "\n",
    "#     a = word2VecRNN()\n",
    "#     a.train(0.00007)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proprietary-bride",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
